{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EM2024007370\\Proyectos\\ESG_TRI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import pdfplumber\n",
    "import openpyxl\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch\n",
    "import faiss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 694\n",
      "First chunk length (words): 500\n"
     ]
    }
   ],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        text = ''\n",
    "        for page in pdf.pages:\n",
    "            text += page.extract_text() + '\\n'\n",
    "    return text\n",
    "\n",
    "\n",
    "def chunk_text(text, chunk_size=100, overlap=10):\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = min(start + chunk_size, len(words))\n",
    "        chunk = words[start:end]\n",
    "        chunks.append(\" \".join(chunk))\n",
    "        start += (chunk_size - overlap)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "pdf_path = './files/informe_telefonica.pdf'\n",
    "raw_text = extract_text_from_pdf(pdf_path)\n",
    "clean_text = \" \".join(raw_text.split())\n",
    "\n",
    "chunks = chunk_text(clean_text, chunk_size=500, overlap=50)\n",
    "\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "print(\"First chunk length (words):\", len(chunks[0].split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EM2024007370\\Proyectos\\ESG_TRI\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\EM2024007370\\.cache\\huggingface\\hub\\models--ibm-granite--granite-embedding-278m-multilingual. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "embed_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# embed_model = SentenceTransformer('nvidia/NV-Embed-v2', trust_remote_code=True)\n",
    "# embed_model.max_seq_length = 32768\n",
    "# embed_model.tokenizer.padding_side=\"right\"\n",
    "# embed_model = SentenceTransformer(\"ibm-granite/granite-embedding-278m-multilingual\")\n",
    "\n",
    "\n",
    "# Genera embeddings para cada chunk\n",
    "chunk_embeddings = embed_model.encode(chunks, convert_to_numpy=True)\n",
    "\n",
    "# Crea un índice FAISS (flat, sin compresión, para simplicidad)\n",
    "dimension = chunk_embeddings.shape[1]  # dimensión de cada vector\n",
    "faiss_index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Añade los embeddings al índice\n",
    "faiss_index.add(chunk_embeddings)\n",
    "\n",
    "# Guardamos los chunks en una lista para referencia\n",
    "chunk_data = chunks  # chunk_data[i] corresponde a chunk_embeddings[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# np.save(\"chunk_data.npy\", chunk_data, allow_pickle=True)\n",
    "\n",
    "\n",
    "# chunk_embeddings = np.load(\"chunk_embeddings.npy\")\n",
    "# dimension = chunk_embeddings.shape[1]\n",
    "# faiss_index = faiss.IndexFlatL2(dimension)\n",
    "# faiss_index.add(chunk_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_path = \"./files/Inputs_TRI.xlsx\"\n",
    "wb = openpyxl.load_workbook(excel_path)\n",
    "ws = wb.active\n",
    "\n",
    "\n",
    "rows_to_fill = list(ws.iter_rows(min_row=2,\n",
    "                                 max_col=5,  # hasta la columna E\n",
    "                                 values_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\EM2024007370\\Proyectos\\ESG_TRI\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\EM2024007370\\.cache\\huggingface\\hub\\models--google--flan-t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "# model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_name = \"google/flan-t5-large\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "def answer_questions(question, context, max_length=128):\n",
    "    # prompt = f\"question: {question}  context: {context}\"\n",
    "    prompt = (\n",
    "        f\"question: {question} \"\n",
    "        \"Please answer concisely.\"\n",
    "        \"Do not copy the entire context verbatim.\"\n",
    "        f\" context: {context}\"\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
    "    outputs = llm_model.generate(**inputs, max_length=max_length, num_beams=2, early_stopping=True)\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True) \n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_chunks(query, top_k=5):\n",
    "    # Generar embedding de la query\n",
    "    query_vector = embed_model.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Buscar en el índice\n",
    "    distances, indices = faiss_index.search(query_vector, top_k)\n",
    "    # indices es un array con los índices de los top_k vectores más cercanos\n",
    "    \n",
    "    relevant_chunks = [chunk_data[i] for i in indices[0]]\n",
    "\n",
    "    # print(\"\\nRELEVANT CHUNK 1: \", relevant_chunks[0])\n",
    "    # print(\"\\nRELEVANT CHUNK 2: \", relevant_chunks[1])\n",
    "    # print(\"\\nRELEVANT CHUNK 3: \", relevant_chunks[2])\n",
    "    # print(\"\\nRELEVANT CHUNK 4: \", relevant_chunks[3])\n",
    "    # print(\"\\nRELEVANT CHUNK 5: \", relevant_chunks[4])\n",
    "    \n",
    "    return relevant_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pregunta:  Encuentra esta información (Strategy, Carbon pricing): Internal CO2 price \n",
      "Answer:  tCOe 1.811.155 1.329.268 536.737 353.346 337.119 -81 % 2 Alcance 1 + 2 (localización) tCO2e 2.155.701 1.993.719 1.395.404 1.133.998 1.158.997 -46 % Emisiones compensadas3 tCOe 63.018 35.537 33.711 NA 2 Alcance 34 tCOe 2.855.5445 2.855.544 2.072.159 1.930.051 1.970.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prueba para una sola pregunta\n",
    "\n",
    "row = rows_to_fill[7]\n",
    "tipo_cell, bloque_cell, subbloque_cell, definicion_cell, valor_cell = row\n",
    "definicion = definicion_cell.value\n",
    "\n",
    "\n",
    "question = f\"Encuentra esta información ({bloque_cell.value}, {subbloque_cell.value}): {definicion}\"\n",
    "query = f\"({bloque_cell.value}, {subbloque_cell.value}) {definicion}\"\n",
    "\n",
    "# Obtener los chunks relevantes\n",
    "relevant_chunks = get_relevant_chunks(query, top_k=10)\n",
    "context_for_llm = \" \".join(relevant_chunks)\n",
    "\n",
    "# Llamar al LLM con la pregunta + contexto\n",
    "answer = answer_questions(question, context_for_llm)\n",
    "\n",
    "\n",
    "print(\"\\nPregunta: \", question, \"\\nAnswer: \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for row in rows_to_fill:\n",
    "    tipo_cell, bloque_cell, subbloque_cell, definicion_cell, valor_cell = row\n",
    "    \n",
    "    definicion = definicion_cell.value\n",
    "    if not definicion:\n",
    "        continue\n",
    "    \n",
    "    question = f\"{definicion} ¿Cuál es el valor según el documento?\"\n",
    "    \n",
    "    # Obtener los chunks relevantes\n",
    "    relevant_chunks = get_relevant_chunks(question, top_k=3)\n",
    "    context_for_llm = \" \".join(relevant_chunks)\n",
    "    \n",
    "    # Llamar al LLM con la pregunta + contexto\n",
    "    answer = answer_questions(question, context_for_llm)\n",
    "    \n",
    "    # Guardar la respuesta en la celda correspondiente\n",
    "    valor_cell.value = answer\n",
    "\n",
    "# Guardamos los cambios al excel\n",
    "wb.save(\"datos_completados.xlsx\")\n",
    "\n",
    "print(\"Done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
